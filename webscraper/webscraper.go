package webscraper

import (
	"fmt"
	"strings"
	"sync"
	"time"

	"github.com/gocolly/colly/v2"
)

// scrapeHeadlines busca as principais not√≠cias de um canal de not√≠cias
func scrapeHeadlines(url string, siteName string, wg *sync.WaitGroup, results chan<- string) {
	defer wg.Done()

	// Cria um novo coletor
	c := colly.NewCollector()

	// Vari√°vel para t√≠tulo
	var title string

	// Define o que fazer quando o t√≠tulo das not√≠cias for encontrado
	c.OnHTML("h3, h2, .headline, .article__title", func(e *colly.HTMLElement) {
		title = e.Text
	})

	// Define o que fazer em caso de erro
	c.OnError(func(r *colly.Response, err error) {
		results <- fmt.Sprintf("Erro ao acessar '%s': %v", url, err)
	})

	// Visita o URL
	err := c.Visit(url)
	if err != nil {
		results <- fmt.Sprintf("Erro ao acessar '%s': %v", url, err)
	}

	// Se o t√≠tulo estiver presente, envia o dado
	if title != "" {
		// Remove espa√ßos extras
		title = strings.TrimSpace(title)
		results <- fmt.Sprintf("%s - %s", siteName, title)
	}
}

// Run executa o exemplo de web scraping buscando not√≠cias de sites populares brasileiros
func Run() {
	// Lista de URLs de sites de not√≠cias e seus nomes
	newsSites := []struct {
		url      string
		siteName string
	}{
		{"https://g1.globo.com/", "G1 - Globo"},                 // G1 Globo
		{"https://www.folha.uol.com.br/", "Folha de S. Paulo"},  // Folha de S. Paulo
		{"https://www.uol.com.br/", "UOL"},                      // UOL
		{"https://www.estadao.com.br/", "O Estado de S. Paulo"}, // O Estado de S. Paulo
		{"https://www.valor.com.br/", "Valor Econ√¥mico"},        // Valor Econ√¥mico
		{"https://www.r7.com/", "R7"},                           // R7
	}

	// Canal para coletar os resultados das not√≠cias
	results := make(chan string, len(newsSites)*10)

	// WaitGroup para aguardar todas as goroutines
	var wg sync.WaitGroup

	// Marca o tempo de in√≠cio
	startTime := time.Now()

	// Inicia a coleta de dados de forma concorrente
	for _, site := range newsSites {
		wg.Add(1) // Incrementa o contador do WaitGroup
		go scrapeHeadlines(site.url, site.siteName, &wg, results)
	}

	// Goroutine para fechar o canal ap√≥s todas as goroutines terminarem
	go func() {
		wg.Wait()
		close(results)
	}()

	// Exibe os resultados enquanto recebe dados do canal
	fmt.Println("üîç Buscando as principais not√≠cias...")

	// Exibe at√© 5 not√≠cias de cada site
	seen := make(map[string]bool)
	for result := range results {
		if len(seen) < 5 {
			if !seen[result] {
				// Exibe as not√≠cias de forma organizada
				fmt.Println("----------------------------------------------------")
				fmt.Println(result)
				fmt.Println("----------------------------------------------------")
				seen[result] = true
			}
		}
	}

	// Marca o tempo de fim e exibe o tempo total
	elapsedTime := time.Since(startTime)
	fmt.Printf("\nTempo total de execu√ß√£o: %s\n", elapsedTime)
}
